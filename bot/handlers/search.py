"""Search command handler."""

import asyncio
import time
from typing import Optional

from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import CommandHandler, ContextTypes, CallbackQueryHandler

from config.logging_config import get_logger
from database.session import get_db
from database.repositories.vacancy_repository import VacancyRepository
from database.repositories.user_repository import UserRepository
from database.repositories.filter_repository import FilterRepository
from parsers.gsz_parser import GszParser
from utils.helpers import format_vacancy_message

logger = get_logger(__name__)

# Store search results in context for pagination
SEARCH_RESULTS_KEY = "search_results"
CURRENT_PAGE_KEY = "current_page"
SEARCH_PARAMS_KEY = "search_params"  # Store search parameters for lazy loading
VACANCIES_PER_PAGE = 20  # Show 20 vacancies per page


async def search_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle /search command."""
    user = update.effective_user
    if not user:
        return

    # Parse command arguments
    args = context.args
    profession = None
    city = None
    company_name = None

    if args:
        # Parse arguments: /search –ø—Ä–æ—Ñ–µ—Å—Å–∏—è [–≥–æ—Ä–æ–¥]
        # Simple approach: if 2+ arguments, last is city, rest is profession
        args_list = list(args)
        
        # Common city/region keywords in Russian/Belarusian
        city_keywords = ["–≤", "–≥–æ—Ä–æ–¥", "–≥.", "–æ–±–ª–∞—Å—Ç—å", "–æ–±–ª.", "—Ä–∞–π–æ–Ω", "—Ä-–Ω"]
        
        # Check if there's a keyword separator (e.g., "–≤", "–≥–æ—Ä–æ–¥")
        keyword_index = None
        for i, arg in enumerate(args_list):
            if arg.lower() in city_keywords:
                keyword_index = i
                break
        
        if keyword_index is not None:
            # Format: /search –ø—Ä–æ—Ñ–µ—Å—Å–∏—è –≤ –≥–æ—Ä–æ–¥
            profession = " ".join(args_list[:keyword_index])
            if keyword_index + 1 < len(args_list):
                city = " ".join(args_list[keyword_index + 1:])
        elif len(args_list) >= 2:
            # Format: /search –ø—Ä–æ—Ñ–µ—Å—Å–∏—è –≥–æ—Ä–æ–¥
            # Last argument might be city, but check if it's a known city name
            # Common Belarusian cities
            known_cities = [
                "–º–∏–Ω—Å–∫", "–º–æ–≥–∏–ª–µ–≤", "–≥–æ–º–µ–ª—å", "–±—Ä–µ—Å—Ç", "–≥—Ä–æ–¥–Ω–æ", "–≤–∏—Ç–µ–±—Å–∫",
                "–±–æ–±—Ä—É–π—Å–∫", "–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏", "–ø–∏–Ω—Å–∫", "–æ—Ä—à–∞", "–º–æ–∑—ã—Ä—å", "—Å–æ–ª–∏–≥–æ—Ä—Å–∫",
                "–Ω–æ–≤–æ–ø–æ–ª–æ—Ü–∫", "–ª–∏–¥–∞", "–º–æ–ª–æ–¥–µ—á–Ω–æ", "–ø–æ–ª–æ—Ü–∫", "—Å–ª–æ–Ω–∏–º", "–∫–æ–±—Ä–∏–Ω",
                "–≤–æ–ª–∫–æ–≤—ã—Å–∫", "–∫–∞–ª–∏–Ω–∫–æ–≤–∏—á–∏", "—Å–≤–µ—Ç–ª–æ–≥–æ—Ä—Å–∫", "—Ä–µ—á–∏—Ü–∞", "–∂–ª–æ–±–∏–Ω",
                "—Å–ª—É—Ü–∫", "–ª–µ–ø–µ–ª—å", "–∫–ª–∏–º–æ–≤–∏—á–∏", "—Ä–æ–≥–∞—á–µ–≤", "—á–∞—É—Å—ã", "—á–∞—à–Ω–∏–∫–∏"
            ]
            
            last_arg_lower = args_list[-1].lower()
            # Check if last argument is a known city
            if last_arg_lower in known_cities or any(city in last_arg_lower for city in known_cities):
                # Last argument is a city
                profession = " ".join(args_list[:-1])
                city = args_list[-1]
            else:
                # Last argument is part of profession (e.g., "–ø–æ–¥—Å–æ–±–Ω—ã–π —Ä–∞–±–æ—á–∏–π")
                profession = " ".join(args_list)
                city = None
        else:
            # Single argument - profession only
            profession = args_list[0]
    else:
        # No arguments - show filters or ask for search parameters
        try:
            db = next(get_db())
            user_repo = UserRepository(db)
            filter_repo = FilterRepository(db)

            db_user = user_repo.get_by_telegram_id(user.id)
            if not db_user:
                await update.message.reply_text(
                    "‚ùå –í—ã –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /start –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏."
                )
                return

            # Get user's active filters
            user_filters = filter_repo.get_active_by_user_id(db_user.id)

            if user_filters:
                # Show filters to choose from
                message = (
                    "üîç <b>–ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π</b>\n\n"
                    "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∏–ª—å—Ç—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\n"
                    "<code>/search –ø–æ–¥—Å–æ–±–Ω—ã–π —Ä–∞–±–æ—á–∏–π</code>\n\n"
                    "<b>–í–∞—à–∏ –∞–∫—Ç–∏–≤–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã:</b>"
                )
                keyboard = []
                for filter_obj in user_filters:
                    filter_desc = filter_obj.name
                    if filter_obj.profession:
                        filter_desc += f" ({filter_obj.profession}"
                        if filter_obj.city:
                            filter_desc += f", {filter_obj.city}"
                        filter_desc += ")"
                    keyboard.append(
                        [
                            InlineKeyboardButton(
                                f"üîç {filter_obj.name}",
                                callback_data=f"search_filter:{filter_obj.id}",
                            )
                        ]
                    )
                keyboard.append(
                    [
                        InlineKeyboardButton(
                            "‚úèÔ∏è –í–≤–µ—Å—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ä—É—á–Ω—É—é",
                            callback_data="search_manual",
                        )
                    ]
                )
                reply_markup = InlineKeyboardMarkup(keyboard)
                await update.message.reply_text(message, parse_mode="HTML", reply_markup=reply_markup)
            else:
                # No filters - ask for manual input
                await update.message.reply_text(
                    "üîç <b>–ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π</b>\n\n"
                    "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É —Ç–∞–∫:\n"
                    "<code>/search –ø–æ–¥—Å–æ–±–Ω—ã–π —Ä–∞–±–æ—á–∏–π</code> - –ø–æ–∏—Å–∫ –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏\n"
                    "<code>/search –ø–æ–¥—Å–æ–±–Ω—ã–π —Ä–∞–±–æ—á–∏–π –ú–∏–Ω—Å–∫</code> - –ø–æ–∏—Å–∫ –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –∏ –≥–æ—Ä–æ–¥—É\n"
                    "<code>/search –ø–ª–æ—Ç–Ω–∏–∫ –≤ –ú–æ–≥–∏–ª–µ–≤</code> - –ø–æ–∏—Å–∫ —Å –ø—Ä–µ–¥–ª–æ–≥–æ–º \"–≤\"\n\n"
                    "üí° <b>–°–æ–≤–µ—Ç:</b> –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∏–ª—å—Ç—Ä —á–µ—Ä–µ–∑ /add_filter –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–º –ø–æ–∏—Å–∫–∞–º.",
                    parse_mode="HTML",
                )
            return
        except Exception as e:
            logger.error(f"Error in search_command: {e}", exc_info=True)
            await update.message.reply_text(
                "üîç <b>–ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π</b>\n\n"
                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É —Ç–∞–∫:\n"
                "<code>/search –ø–æ–¥—Å–æ–±–Ω—ã–π —Ä–∞–±–æ—á–∏–π</code> - –ø–æ–∏—Å–∫ –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏\n"
                "<code>/search –ø–æ–¥—Å–æ–±–Ω—ã–π —Ä–∞–±–æ—á–∏–π –ú–∏–Ω—Å–∫</code> - –ø–æ–∏—Å–∫ –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –∏ –≥–æ—Ä–æ–¥—É",
                parse_mode="HTML",
            )
            return

    # Show loading message
    loading_msg = await update.message.reply_text("‚è≥ –ò—â—É –≤–∞–∫–∞–Ω—Å–∏–∏...")

    # Progress tracking
    last_progress_update = 0

    async def update_progress(current_page: int, total_pages: int, found_count: int) -> None:
        """Update progress message."""
        nonlocal last_progress_update
        
        # Update progress more frequently for better UX
        # Always update on first page (page 0) to show initial progress immediately
        current_time = time.time()
        time_since_last = current_time - last_progress_update
        
        # Update immediately on first page, then every 1 second minimum
        if current_page == 0:
            # First page - update immediately
            pass
        elif time_since_last < 1.0:
            # Too soon since last update
            return
        
        last_progress_update = current_time
        
        # Calculate progress percentage
        if total_pages > 0:
            progress_pct = min(int((current_page / total_pages) * 100), 100)
        else:
            progress_pct = 0
        
        # Create progress bar with more visual elements
        bar_length = 12
        filled = int(bar_length * progress_pct / 100)
        bar = "‚ñà" * filled + "‚ñë" * (bar_length - filled)
        
        # Add spinner animation based on page number
        spinner = ["‚è≥", "üîç", "üìã", "üîé"][current_page % 4]
        
        progress_text = (
            f"{spinner} <b>–ò—â—É –≤–∞–∫–∞–Ω—Å–∏–∏...</b>\n\n"
            f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {current_page}/{total_pages if total_pages > 0 else '?'}\n"
            f"üìä –ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {found_count}\n"
            f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: [{bar}] {progress_pct}%\n\n"
            f"<i>–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...</i>"
        )
        
        try:
            await loading_msg.edit_text(progress_text, parse_mode="HTML")
        except Exception as e:
            error_str = str(e).lower()
            if "not modified" not in error_str:
                logger.debug(f"Could not update progress message: {e}")

    try:
        # First, get exact total count of vacancies from the page
        async with GszParser() as parser:
            total_vacancies = await parser.get_total_vacancies_count(profession, city, company_name)
            
            # If exact count not found, estimate from pages
            if total_vacancies is None:
                total_pages = await parser.get_total_pages(profession, city, company_name)
                total_vacancies = total_pages * 20  # ~20 vacancies per page
                logger.info(f"Estimated total vacancies: {total_vacancies} (from {total_pages} pages)")
            else:
                logger.info(f"Found exact total vacancies count: {total_vacancies}")
            
            # Parse first batch of vacancies (first page to show immediately)
            vacancies = await parser.parse_vacancies(
                profession=profession,
                city=city,
                company_name=company_name,
                limit=VACANCIES_PER_PAGE,  # Parse first 20 for immediate display
                progress_callback=update_progress,
            )

        if not vacancies:
            await loading_msg.edit_text(
                f"‚ùå –í–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É <b>\"{profession}\"</b> –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\n\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞.",
                parse_mode="HTML",
            )
            return

        # Save search parameters for lazy loading more pages
        context.user_data[SEARCH_PARAMS_KEY] = {
            "profession": profession,
            "city": city,
            "company_name": company_name,
        }
        
        # Save vacancies to database
        db = next(get_db())
        vacancy_repo = VacancyRepository(db)
        saved_count = 0

        for vacancy_data in vacancies:
            # Check if vacancy already exists
            existing = vacancy_repo.get_by_external_id_and_source(
                vacancy_data["external_id"], vacancy_data["source"]
            )
            if not existing:
                vacancy_repo.create(vacancy_data)
                saved_count += 1

        # Store results in context for pagination
        context.user_data[SEARCH_RESULTS_KEY] = vacancies
        context.user_data[CURRENT_PAGE_KEY] = 0
        context.user_data["total_vacancies"] = total_vacancies  # Exact or estimated count
        context.user_data["parsed_pages"] = 1  # Track how many pages we've parsed

        # Show final message briefly before showing results
        final_text = (
            f"‚úÖ <b>–ù–∞–π–¥–µ–Ω–æ: {total_vacancies} –≤–∞–∫–∞–Ω—Å–∏–π</b>\n"
            f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π\n"
            "üíæ –°–æ—Ö—Ä–∞–Ω—è—é –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö..."
        )
        await loading_msg.edit_text(final_text, parse_mode="HTML")
        await asyncio.sleep(0.8)  # Show final message for 0.8 seconds

        # Show first batch of 20 vacancies
        await show_search_results_batch(update, context, batch=0, message=loading_msg)

    except Exception as e:
        logger.error(f"Error searching vacancies: {e}", exc_info=True)
        await loading_msg.edit_text(
            "‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∞–∫–∞–Ω—Å–∏–π.\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞."
        )


async def show_search_results_batch(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    batch: int = 0,
    message=None,
) -> None:
    """Show search results in batches of 20 vacancies."""
    vacancies = context.user_data.get(SEARCH_RESULTS_KEY, [])
    if not vacancies:
        return

    total_vacancies = context.user_data.get("total_vacancies", len(vacancies))
    parsed_pages = context.user_data.get("parsed_pages", 1)
    
    # Calculate batch range
    start_idx = batch * VACANCIES_PER_PAGE
    end_idx = min(start_idx + VACANCIES_PER_PAGE, len(vacancies))
    batch_vacancies = vacancies[start_idx:end_idx]
    
    if not batch_vacancies:
        # Need to load more vacancies
        await load_more_vacancies(update, context, batch, message)
        return

    # Format message with batch info
    total_batches = (len(vacancies) + VACANCIES_PER_PAGE - 1) // VACANCIES_PER_PAGE
    message_text = (
        f"üìä <b>–ù–∞–π–¥–µ–Ω–æ: {total_vacancies} –≤–∞–∫–∞–Ω—Å–∏–π</b>\n"
        f"üìã –ü–æ–∫–∞–∑–∞–Ω–æ: {start_idx + 1}-{end_idx} –∏–∑ {total_vacancies}\n"
        f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(vacancies)} –∏–∑ {total_vacancies}\n\n"
    )
    
    # Show up to 20 vacancies in compact format
    for i, vacancy in enumerate(batch_vacancies[:20], start=start_idx + 1):
        position = vacancy.get("position", "–ù–µ —É–∫–∞–∑–∞–Ω–æ")
        company = vacancy.get("company_name", "–ù–µ —É–∫–∞–∑–∞–Ω–æ")
        address = vacancy.get("company_address", "")
        salary = vacancy.get("salary", "")
        
        message_text += f"<b>{i}. {position}</b>\n"
        message_text += f"üè¢ {company}\n"
        if address:
            message_text += f"üìç {address}\n"
        if salary:
            message_text += f"üí∞ {salary}\n"
        message_text += "\n"

    # Create pagination keyboard
    keyboard = []
    nav_buttons = []

    if batch > 0:
        nav_buttons.append(
            InlineKeyboardButton("‚óÄÔ∏è –ü—Ä–µ–¥—ã–¥—É—â–∏–µ 20", callback_data=f"search_batch:{batch-1}")
        )

    nav_buttons.append(
        InlineKeyboardButton(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {batch + 1}", callback_data="noop")
    )

    # Check if we have more vacancies or can load more
    has_more = end_idx < len(vacancies) or len(vacancies) < total_vacancies
    if has_more:
        nav_buttons.append(
            InlineKeyboardButton("–°–ª–µ–¥—É—é—â–∏–µ 20 ‚ñ∂Ô∏è", callback_data=f"search_batch:{batch+1}")
        )

    if nav_buttons:
        keyboard.append(nav_buttons)
    
    # Add button to load all remaining vacancies
    if len(vacancies) < total_vacancies:
        remaining = total_vacancies - len(vacancies)
        keyboard.append([
            InlineKeyboardButton(
                f"üì• –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ ({remaining} –æ—Å—Ç–∞–ª–æ—Å—å)",
                callback_data="search_load_all"
            )
        ])

    reply_markup = InlineKeyboardMarkup(keyboard) if keyboard else None

    try:
        if message:
            await message.edit_text(message_text, parse_mode="HTML", reply_markup=reply_markup)
        else:
            query = update.callback_query
            if query:
                await query.edit_message_text(
                    message_text, parse_mode="HTML", reply_markup=reply_markup
                )
    except Exception as e:
        # Handle "Message is not modified" error gracefully
        error_str = str(e).lower()
        if "not modified" in error_str or "message is not modified" in error_str:
            logger.debug("Message not modified - content is the same, ignoring")
            # Just answer the callback query
            if update.callback_query:
                await update.callback_query.answer()
        else:
            logger.error(f"Error updating message: {e}", exc_info=True)
            raise


async def load_more_vacancies(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    batch: int,
    message=None,
) -> None:
    """Load more vacancies for the current search."""
    query = update.callback_query
    if query:
        await query.answer()
    
    search_params = context.user_data.get(SEARCH_PARAMS_KEY)
    if not search_params:
        if query:
            await query.edit_message_text(
                "‚ùå –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ù–∞—á–Ω–∏—Ç–µ –Ω–æ–≤—ã–π –ø–æ–∏—Å–∫."
            )
        return

    # Show loading message
    loading_text = "‚è≥ –ó–∞–≥—Ä—É–∂–∞—é —Å–ª–µ–¥—É—é—â–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏..."
    try:
        if message:
            await message.edit_text(loading_text)
        elif query:
            await query.edit_message_text(loading_text)
    except Exception as e:
        logger.debug(f"Could not update loading message: {e}")

    try:
        parsed_pages = context.user_data.get("parsed_pages", 1)
        current_vacancies = context.user_data.get(SEARCH_RESULTS_KEY, [])
        
        # Parse next page (page = parsed_pages + 1)
        next_page = parsed_pages + 1
        
        # Build URL for next page manually to parse only that page
        async with GszParser() as parser:
            # Parse only the next page
            url = parser.build_search_url(
                profession=search_params["profession"],
                city=search_params["city"],
                company_name=search_params["company_name"],
                page=next_page,
            )
            
            html = await parser._fetch_page(url)
            if not html:
                if query:
                    await query.edit_message_text(
                        "‚úÖ –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã."
                    )
                return
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, "html.parser")
            vacancy_items = soup.find_all("div", class_="job-block")
            
            if len(vacancy_items) == 0:
                if query:
                    await query.edit_message_text(
                        "‚úÖ –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã."
                    )
                return
            
            # Parse vacancies from this page
            next_page_vacancies = []
            for item in vacancy_items:
                vacancy = await parser._parse_vacancy_item(item)
                if vacancy:
                    # Apply city filter if needed
                    if search_params.get("city") and search_params["city"]:
                        address = vacancy.get("company_address", "").lower()
                        city_lower = search_params["city"].lower()
                        # Skip if city filter doesn't match (but only if city is a real city)
                        if len(city_lower) >= 4 and city_lower not in address:
                            # Check city variations
                            city_variations = [
                                city_lower,
                                f"–≥. {city_lower}",
                                f"–≥ {city_lower}",
                            ]
                            matches = any(var in address for var in city_variations)
                            if not matches:
                                continue
                    
                    next_page_vacancies.append(vacancy)
        
        if not next_page_vacancies:
            if query:
                await query.edit_message_text(
                    "‚úÖ –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã."
                )
            return
        
        # Save to database
        db = next(get_db())
        vacancy_repo = VacancyRepository(db)
        
        for vacancy_data in next_page_vacancies:
            existing = vacancy_repo.get_by_external_id_and_source(
                vacancy_data["external_id"], vacancy_data["source"]
            )
            if not existing:
                vacancy_repo.create(vacancy_data)
        
        # Add to existing vacancies
        current_vacancies.extend(next_page_vacancies)
        context.user_data[SEARCH_RESULTS_KEY] = current_vacancies
        context.user_data["parsed_pages"] = parsed_pages + 1
        
        # Show the batch
        await show_search_results_batch(update, context, batch=batch)
        
    except Exception as e:
        logger.error(f"Error loading more vacancies: {e}", exc_info=True)
        if query:
            try:
                await query.edit_message_text(
                    "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–∞–∫–∞–Ω—Å–∏–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                )
            except Exception:
                pass


async def load_all_vacancies(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
) -> None:
    """Load all remaining vacancies."""
    search_params = context.user_data.get(SEARCH_PARAMS_KEY)
    if not search_params:
        await update.callback_query.edit_message_text(
            "‚ùå –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
        )
        return

    await update.callback_query.edit_message_text(
        "‚è≥ –ó–∞–≥—Ä—É–∂–∞—é –≤—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –≤–∞–∫–∞–Ω—Å–∏–∏...\n"
        "–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è."
    )

    try:
        current_vacancies = context.user_data.get(SEARCH_RESULTS_KEY, [])
        parsed_pages = context.user_data.get("parsed_pages", 1)
        
        # Progress callback
        async def update_progress(current_page: int, total_pages: int, found_count: int) -> None:
            try:
                await update.callback_query.edit_message_text(
                    f"‚è≥ –ó–∞–≥—Ä—É–∂–∞—é –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏...\n"
                    f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞: {current_page}\n"
                    f"üìä –ù–∞–π–¥–µ–Ω–æ: {found_count} –≤–∞–∫–∞–Ω—Å–∏–π"
                )
            except:
                pass
        
        # Parse all remaining pages
        async with GszParser() as parser:
            all_vacancies = await parser.parse_vacancies(
                profession=search_params["profession"],
                city=search_params["city"],
                company_name=search_params["company_name"],
                limit=None,  # No limit - get all
                progress_callback=update_progress,
            )
        
        # Save to database
        db = next(get_db())
        vacancy_repo = VacancyRepository(db)
        
        for vacancy_data in all_vacancies:
            existing = vacancy_repo.get_by_external_id_and_source(
                vacancy_data["external_id"], vacancy_data["source"]
            )
            if not existing:
                vacancy_repo.create(vacancy_data)
        
        # Update context
        context.user_data[SEARCH_RESULTS_KEY] = all_vacancies
        # Keep original total count if it exists, otherwise use parsed count
        original_total = context.user_data.get("total_vacancies")
        if original_total:
            context.user_data["total_vacancies"] = max(original_total, len(all_vacancies))
        else:
            context.user_data["total_vacancies"] = len(all_vacancies)
        context.user_data["parsed_pages"] = 999  # Mark as fully parsed
        
        # Show first batch
        await show_search_results_batch(update, context, batch=0)
        
    except Exception as e:
        logger.error(f"Error loading all vacancies: {e}", exc_info=True)
        await update.callback_query.edit_message_text(
            "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤—Å–µ—Ö –≤–∞–∫–∞–Ω—Å–∏–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        )


async def search_batch_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle batch pagination callback for search results."""
    query = update.callback_query
    await query.answer()

    # Extract batch number from callback data
    callback_data = query.data
    if callback_data == "noop":
        return
    
    if callback_data.startswith("search_batch:"):
        batch = int(callback_data.split(":")[1])
        await show_search_results_batch(update, context, batch=batch)
    elif callback_data == "search_load_all":
        await load_all_vacancies(update, context)


async def search_filter_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle search using filter callback."""
    query = update.callback_query
    await query.answer()

    if not query.data or not query.data.startswith("search_filter:"):
        return

    filter_id = int(query.data.split(":")[1])
    user = update.effective_user
    if not user:
        return

    try:
        db = next(get_db())
        user_repo = UserRepository(db)
        filter_repo = FilterRepository(db)

        db_user = user_repo.get_by_telegram_id(user.id)
        if not db_user:
            await query.edit_message_text("‚ùå –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return

        filter_obj = filter_repo.get_by_id(filter_id)
        if not filter_obj or filter_obj.user_id != db_user.id:
            await query.edit_message_text("‚ùå –§–∏–ª—å—Ç—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return

        # Use filter parameters for search
        profession = filter_obj.profession
        city = filter_obj.city
        company_name = filter_obj.company_name

        # Show loading message
        loading_msg = await query.edit_message_text(
            f"‚è≥ –ò—â—É –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É <b>\"{filter_obj.name}\"</b>...",
            parse_mode="HTML",
        )

        # Progress tracking
        last_progress_update = 0

        async def update_progress(current_page: int, total_pages: int, found_count: int) -> None:
            """Update progress message."""
            nonlocal last_progress_update
            
            # Update progress every page or every 3 seconds
            import time
            current_time = time.time()
            if current_time - last_progress_update < 3 and current_page > 0:
                return  # Skip if updated recently
            
            last_progress_update = current_time
            
            # Calculate progress percentage
            if total_pages > 0:
                progress_pct = min(int((current_page / total_pages) * 100), 100)
            else:
                progress_pct = 0
            
            # Create progress bar
            bar_length = 10
            filled = int(bar_length * progress_pct / 100)
            bar = "‚ñà" * filled + "‚ñë" * (bar_length - filled)
            
            progress_text = (
                f"‚è≥ <b>–ò—â—É –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É \"{filter_obj.name}\"...</b>\n\n"
                f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞: {current_page}/{total_pages if total_pages > 0 else '?'}\n"
                f"üìä –ù–∞–π–¥–µ–Ω–æ: {found_count} –≤–∞–∫–∞–Ω—Å–∏–π\n"
                f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: [{bar}] {progress_pct}%"
            )
            
            try:
                await loading_msg.edit_text(progress_text, parse_mode="HTML")
            except Exception as e:
                logger.debug(f"Could not update progress message: {e}")

        # First, get exact total count of vacancies from the page
        async with GszParser() as parser:
            total_vacancies = await parser.get_total_vacancies_count(profession, city, company_name)
            
            # If exact count not found, estimate from pages
            if total_vacancies is None:
                total_pages = await parser.get_total_pages(profession, city, company_name)
                total_vacancies = total_pages * 20  # ~20 vacancies per page
                logger.info(f"Estimated total vacancies: {total_vacancies} (from {total_pages} pages)")
            else:
                logger.info(f"Found exact total vacancies count: {total_vacancies}")
            
            # Parse first batch of vacancies (first page to show immediately)
            vacancies = await parser.parse_vacancies(
                profession=profession,
                city=city,
                company_name=company_name,
                limit=VACANCIES_PER_PAGE,  # Parse first 20 for immediate display
                progress_callback=update_progress,
            )

        if not vacancies:
            await loading_msg.edit_text(
                f"‚ùå –í–∞–∫–∞–Ω—Å–∏–∏ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É <b>\"{filter_obj.name}\"</b> –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\n\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–ª—å—Ç—Ä–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π —Ñ–∏–ª—å—Ç—Ä.",
                parse_mode="HTML",
            )
            return

        # Save search parameters for lazy loading more pages
        context.user_data[SEARCH_PARAMS_KEY] = {
            "profession": profession,
            "city": city,
            "company_name": company_name,
        }

        # Save vacancies to database
        vacancy_repo = VacancyRepository(db)
        saved_count = 0

        for vacancy_data in vacancies:
            existing = vacancy_repo.get_by_external_id_and_source(
                vacancy_data["external_id"], vacancy_data["source"]
            )
            if not existing:
                vacancy_repo.create(vacancy_data)
                saved_count += 1

        # Store results in context for pagination
        context.user_data[SEARCH_RESULTS_KEY] = vacancies
        context.user_data[CURRENT_PAGE_KEY] = 0
        context.user_data["total_vacancies"] = total_vacancies  # Exact or estimated count
        context.user_data["parsed_pages"] = 1

        # Show first batch of 20 vacancies
        await show_search_results_batch(update, context, batch=0, message=loading_msg)

    except Exception as e:
        logger.error(f"Error in search_filter_callback: {e}", exc_info=True)
        await query.edit_message_text(
            "‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∞–∫–∞–Ω—Å–∏–π.\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ñ–∏–ª—å—Ç—Ä."
        )


async def search_manual_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle manual search input callback."""
    query = update.callback_query
    await query.answer()

    await query.edit_message_text(
        "üîç <b>–ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π</b>\n\n"
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É —Ç–∞–∫:\n"
        "<code>/search –ø–æ–¥—Å–æ–±–Ω—ã–π —Ä–∞–±–æ—á–∏–π</code> - –ø–æ–∏—Å–∫ –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏\n"
        "<code>/search –ø–æ–¥—Å–æ–±–Ω—ã–π —Ä–∞–±–æ—á–∏–π –ú–∏–Ω—Å–∫</code> - –ø–æ–∏—Å–∫ –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –∏ –≥–æ—Ä–æ–¥—É\n\n"
        "–ò–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ —Ñ–∏–ª—å—Ç—Ä —á–µ—Ä–µ–∑ /add_filter –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞.",
        parse_mode="HTML",
    )


# Handlers
search_handler = CommandHandler("search", search_command)
search_page_handler = CallbackQueryHandler(search_batch_callback, pattern="^search_batch:|^search_load_all$")
search_filter_handler = CallbackQueryHandler(search_filter_callback, pattern="^search_filter:")
search_manual_handler = CallbackQueryHandler(search_manual_callback, pattern="^search_manual$")